{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'evaluation_helper'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5a3f3809743e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mevaluation_helper\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Read in data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'evaluation_helper'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import evaluation_helper as eval\n",
    "\n",
    "# Read in data\n",
    "\n",
    "features = pd.read_csv('../data/answer.csv', sep=';', nrows=1000, skiprows=[i for i in range(1, 1)])\n",
    "#features = pd.read_csv('../data/answer.csv', sep=';')\n",
    "\n",
    "features.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing labels\n",
    "import numpy as np\n",
    "questions = np.array(features['place_asked'])\n",
    "answers = np.array(features['place_answered'])\n",
    "labels = np.equal(questions, answers)\n",
    "labels = list(map(lambda x: 1 if x else 0, labels))\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing answers from features\n",
    "features = features.drop('place_answered', axis=1)\n",
    "# TODO parse date\n",
    "features = features.drop('inserted', axis=1)\n",
    "# Removing non-important or missing features\n",
    "features = features.drop('place_map', axis=1)\n",
    "features = features.drop('ip_country', axis=1)\n",
    "features = features.drop('ip_id', axis=1)\n",
    "# Formatting float for the output\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "# Parsing options to number of options\n",
    "features['options'] = features['options'].apply(lambda x: len(x[1:-1].split(\",\")))\n",
    "\n",
    "features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Saving feature names for later use\n",
    "feature_list = list(features.columns)\n",
    "# Convert to numpy array\n",
    "features = np.array(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)\n",
    "\n",
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The baseline predictions are always correct answers\n",
    "averages = np.array([1 for x in range(0, len(test_labels))])\n",
    "# Compute metrics for baseline\n",
    "print('Accuracy: ', eval.accuracy(test_labels, averages))\n",
    "print('RMSE: ', eval.rmse(averages, test_labels))\n",
    "print('AUC: ', eval.auc(list(map(int,test_labels)), averages))\n",
    "print('Pearson: ', eval.pearson(averages, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Instantiate model with 1000 decision trees and deterministic random_state\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "# Train the model on training data\n",
    "rf.fit(train_features, train_labels)\n",
    "\n",
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(test_features)\n",
    "# Compute metrics for test set\n",
    "print('Accuracy: ', eval.accuracy(test_labels, averages))\n",
    "print('RMSE: ', eval.rmse(predictions, test_labels))\n",
    "print('AUC: ', eval.auc(list(map(int,test_labels)), predictions))\n",
    "print('Pearson: ', eval.pearson(predictions, test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"##############################FEATURES IMPORTANCE###################################\")\n",
    "# Get numerical feature importances\n",
    "importances = list(rf.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances\n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
